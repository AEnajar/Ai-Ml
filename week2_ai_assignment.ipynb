{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c73531c",
   "metadata": {},
   "source": [
    "# Week 2 - AI/ML Assignment (Web Scraping & Text Preprocessing)\n",
    "\n",
    "This week focuses on:\n",
    "- Web scraping job descriptions\n",
    "- Cleaning and preparing text for NLP tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Scrape Job Descriptions\n",
    "We'll use `requests` and `BeautifulSoup` to extract text from a sample job site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Example job board URL (you can replace with another)\n",
    "url = 'https://remoteok.com/remote-data-jobs'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract visible text\n",
    "raw_text = soup.get_text()\n",
    "\n",
    "# Display a preview\n",
    "print(\"Raw scraped text (first 1000 chars):\")\n",
    "print(raw_text[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda1a65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Text Cleaning\n",
    "\n",
    "Clean and standardize the raw text for NLP analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879688c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Lowercase\n",
    "    return text\n",
    "\n",
    "cleaned = clean_text(raw_text)\n",
    "print(\"Cleaned text (first 500 chars):\")\n",
    "print(cleaned[:500])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
